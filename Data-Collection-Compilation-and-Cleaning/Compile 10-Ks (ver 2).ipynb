{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import os\n",
    "\n",
    "\n",
    "# path=\"/Users/EstebanArgudo/Documents/SECfiles/Files\"\n",
    "# cwd=os.getcwd\n",
    "# os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josh\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "form = '10-K'\n",
    "year=2009\n",
    "keyword='LongTermDebtNoncurrent'\n",
    "variable_list=['LongTermDebtNoncurrent']\n",
    "\n",
    "# Looping through the years\n",
    "\n",
    "if (year>= 2009):\n",
    "    q1_sub = pd.read_csv(str(year)+'q1sub.tsv',sep='\\t',encoding = \"ISO-8859-1\")\n",
    "    q1_num = pd.read_csv(str(year)+'q1num.tsv',sep='\\t',encoding = \"ISO-8859-1\")\n",
    "\n",
    "q2_sub = pd.read_csv(str(year)+'q2sub.tsv',sep='\\t',encoding = \"ISO-8859-1\")\n",
    "q2_num = pd.read_csv(str(year)+'q2num.tsv',sep='\\t',encoding = \"ISO-8859-1\")\n",
    "q3_sub = pd.read_csv(str(year)+'q3sub.tsv',sep='\\t',encoding = \"ISO-8859-1\")\n",
    "q3_num = pd.read_csv(str(year)+'q3num.tsv',sep='\\t',encoding = \"ISO-8859-1\")\n",
    "q4_sub = pd.read_csv(str(year)+'q4sub.tsv',sep='\\t',encoding = \"ISO-8859-1\")\n",
    "q4_num = pd.read_csv(str(year)+'q4num.tsv',sep='\\t',encoding = \"ISO-8859-1\")\n",
    "\n",
    "companies=q2_sub.append(q3_sub).append(q4_sub)\n",
    "values=q2_num.append(q3_num).append(q4_num)\n",
    "\n",
    "if (year>= 2009):\n",
    "    companies=companies.append(q1_sub)\n",
    "    values=values.append(q1_num)\n",
    "\n",
    "if (year==2018):\n",
    "    companies=q1_sub\n",
    "    values=q1_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# form = '10-K'\n",
    "# year=2016\n",
    "# keyword='ResearchAndDevelopmentExpense'\n",
    "# variable_list=['ResearchAndDevelopmentExpense']\n",
    "\n",
    "# # Looping through the years\n",
    "\n",
    "# if (year> 2009):\n",
    "#     q1_sub = pd.read_csv(str(year)+'q1sub.csv',encoding = \"ISO-8859-1\")\n",
    "#     q1_num = pd.read_csv(str(year)+'q1num.csv',encoding = \"ISO-8859-1\")\n",
    "\n",
    "# q2_sub = pd.read_csv(str(year)+'q2sub.csv', encoding = \"ISO-8859-1\")\n",
    "# q2_num = pd.read_csv(str(year)+'q2num.csv', encoding = \"ISO-8859-1\")\n",
    "# q3_sub = pd.read_csv(str(year)+'q3sub.csv', encoding = \"ISO-8859-1\")\n",
    "# q3_num = pd.read_csv(str(year)+'q3num.csv', encoding = \"ISO-8859-1\")\n",
    "# q4_sub = pd.read_csv(str(year)+'q4sub.csv', encoding = \"ISO-8859-1\")\n",
    "# q4_num = pd.read_csv(str(year)+'q4num.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "# companies=q2_sub.append(q3_sub).append(q4_sub)\n",
    "# values=q2_num.append(q3_num).append(q4_num)\n",
    "\n",
    "# if (year> 2009):\n",
    "#     companies=companies.append(q1_sub)\n",
    "#     values=values.append(q1_num)\n",
    "\n",
    "# if (year==2018):\n",
    "#     companies=q1_sub\n",
    "#     values=q1_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Complete num file for for 4 quarters with values sorted\n",
    "values.sort_values(['adsh', 'tag','ddate', 'coreg','value'], inplace=True)\n",
    "count_values=len(values.index)\n",
    "count_values\n",
    "\n",
    "# Complete sub file for for 4 quarters with values sorted\n",
    "companies.sort_values(['adsh','period','form'], inplace=True)\n",
    "count_companies=len(companies.index)\n",
    "count_companies\n",
    "\n",
    "# Add the cik field to values by merging on adsh\n",
    "cik_merge=companies[['adsh','cik','fy','sic','ein','period','form']]\n",
    "val=values.merge(cik_merge,on='adsh',how='left')\n",
    "len(val)\n",
    "ciknull=val[val['cik'].isnull()]\n",
    "count_ciknull=len(ciknull.index)\n",
    "count_ciknull\n",
    "\n",
    "# Looping through keywords\n",
    "# for keyword in variable_list:\n",
    "\n",
    "# Filter out tha values with tags equal to the keyword\n",
    "val=val[val['tag']==keyword].loc[:,['adsh','cik','ddate','coreg','value','qtrs']]\n",
    "val=val.rename(columns={'value':keyword})\n",
    "count_val=len(val.index)\n",
    "count_val\n",
    "\n",
    "############ GET VALUES FROM NUM FILE MATCHING ADSH FROM SUB FILE ##############\n",
    "comp_form = companies[companies.form == form]\n",
    "count_total = len(comp_form.index)\n",
    "count_total\n",
    "\n",
    "############### NCIK==1\n",
    "con=comp_form[comp_form['nciks']==1][['adsh','cik','fy','sic','ein','period']]\n",
    "count_con_total = len(con)\n",
    "count_con_total\n",
    "\n",
    "# Add values in NUM to con: Number of entries in con_adsh increase relative to con as multiple values with the same adsh get stacked\n",
    "con_adsh = con.merge(val[['adsh','ddate','coreg','qtrs',keyword]],on='adsh',how='left')\n",
    "len(con_adsh.index)\n",
    "len(con_adsh.drop_duplicates(subset=['adsh'],keep='last'))   # Check\n",
    "\n",
    "# Subcases:\n",
    "#1. No adsh in NUM file (null ddate)\n",
    "con_missing_adsh = con_adsh[con_adsh['ddate'].isnull()]\n",
    "count_con_subcase1 = len(con_missing_adsh.drop_duplicates(subset=['adsh'],keep='last'))\n",
    "count_con_subcase1\n",
    "#2. adsh in NUM file\n",
    "con_nonmissing_adsh = con_adsh[con_adsh['ddate'].isnull()==False]\n",
    "count_con_subcase2 = len(con_nonmissing_adsh.drop_duplicates(subset=['adsh'],keep='last'))\n",
    "count_con_subcase2\n",
    "# Subsubcases\n",
    "# adshs found in NUM file that contain information about the reporting period\n",
    "con_nonmissing_adsh_reportingperiod=con_nonmissing_adsh[con_nonmissing_adsh.period==con_nonmissing_adsh.ddate]\n",
    "# adshs found in NUM file that contain information about periods other than the reporting period\n",
    "con_nonmissing_adsh_otherperiods=con_nonmissing_adsh[con_nonmissing_adsh.period!=con_nonmissing_adsh.ddate]\n",
    "# 2.a. adshs found that ONLY have records corresponding to OTHER periods and not for reporting period\n",
    "con_nonmissing_adsh_otherperiods_only=con_nonmissing_adsh_otherperiods[con_nonmissing_adsh_otherperiods.adsh.isin(con_nonmissing_adsh_reportingperiod.adsh)==False]\n",
    "count_con_subcase2a = len(con_nonmissing_adsh_otherperiods_only.drop_duplicates(subset=['adsh'],keep='last'))\n",
    "count_con_subcase2a\n",
    "# 2.b adshs found that ONLY have records corresponding to the reporting period\n",
    "con_nonmissing_adsh_reportingperiod_only=con_nonmissing_adsh_reportingperiod[con_nonmissing_adsh_reportingperiod.adsh.isin(con_nonmissing_adsh_otherperiods.adsh)==False]\n",
    "count_con_subcase2b = len(con_nonmissing_adsh_reportingperiod_only.drop_duplicates(subset=['adsh'],keep='last'))\n",
    "count_con_subcase2b\n",
    "# 2.c adshs found that have records for BOTH, the reporting period AND other periods\n",
    "con_nonmissing_adsh_both_periods=con_nonmissing_adsh_reportingperiod[con_nonmissing_adsh_reportingperiod.adsh.isin(con_nonmissing_adsh_otherperiods.adsh)]\n",
    "count_con_subcase2c = len(con_nonmissing_adsh_both_periods.drop_duplicates(subset=['adsh'],keep='last'))\n",
    "count_con_subcase2c\n",
    "\n",
    "# Naturally Null values\n",
    "con_nonmissing_adsh_reportingperiod_only_null = con_nonmissing_adsh_reportingperiod_only[con_nonmissing_adsh_reportingperiod_only[keyword].isnull()]\n",
    "con_nonmissing_adsh_both_periods_null = con_nonmissing_adsh_both_periods[con_nonmissing_adsh_both_periods[keyword].isnull()]\n",
    "count_con_subcase2b_null= len(con_nonmissing_adsh_reportingperiod_only_null.drop_duplicates(subset=['adsh'],keep='last'))\n",
    "count_con_subcase2c_null= len(con_nonmissing_adsh_both_periods_null.drop_duplicates(subset=['adsh'],keep='last'))\n",
    "\n",
    "# Revise count of null values since there are cases where:\n",
    "# 1. adsh has some subsidiries reporting null values while other subsidiares have non-null values\n",
    "# 2. adsh has value reported as 0 AND as null.\n",
    "count_con_subcase2b_null = count_con_subcase2b_null - len(con_nonmissing_adsh_reportingperiod_only[con_nonmissing_adsh_reportingperiod_only.adsh.isin(con_nonmissing_adsh_reportingperiod_only_null.adsh)].drop_duplicates(subset=['adsh'],keep='last'))\n",
    "count_con_subcase2c_null = count_con_subcase2c_null - len(con_nonmissing_adsh_both_periods[con_nonmissing_adsh_both_periods.adsh.isin(con_nonmissing_adsh_both_periods_null.adsh)].drop_duplicates(subset=['adsh'],keep='last'))\n",
    "count_con_subcase2b_null\n",
    "count_con_subcase2c_null\n",
    "\n",
    "\n",
    "############### NCIK>1\n",
    "rest=comp_form[comp_form['nciks']!=1][['adsh','cik','sic','ein','fy','period']]\n",
    "count_rest_total = len(rest)\n",
    "count_rest_total\n",
    "# Add values in NUM to rest: Number of entries in rest_val increase relative to rest as multiple values with the same adsh get stacked\n",
    "rest_adsh = rest.merge(val[['adsh','ddate','coreg','qtrs',keyword]],on='adsh',how='left')\n",
    "len(rest_adsh.index)\n",
    "len(rest_adsh.drop_duplicates(subset=['adsh'],keep='last')) # Check\n",
    "# Subcases:\n",
    "#1. No adsh in NUM file (null ddate)\n",
    "rest_missing_adsh = rest_adsh[rest_adsh['ddate'].isnull()]\n",
    "count_rest_subcase1 = len(rest_missing_adsh.drop_duplicates(subset=['adsh'],keep='last'))\n",
    "count_rest_subcase1\n",
    "#2. adsh in NUM file\n",
    "rest_nonmissing_adsh = rest_adsh[rest_adsh['ddate'].isnull()==False]\n",
    "count_rest_subcase2 = len(rest_nonmissing_adsh.drop_duplicates(subset=['adsh'],keep='last'))\n",
    "count_rest_subcase2\n",
    "# Subsubcases\n",
    "# adshs found in NUM file that contain information about the reporting period\n",
    "rest_nonmissing_adsh_reportingperiod=rest_nonmissing_adsh[rest_nonmissing_adsh.period==rest_nonmissing_adsh.ddate]\n",
    "# adshs found in NUM file that contain information about periods other than the reporting period\n",
    "rest_nonmissing_adsh_otherperiods=rest_nonmissing_adsh[rest_nonmissing_adsh.period!=rest_nonmissing_adsh.ddate]\n",
    "# 2.a. adshs found that ONLY have records corresponding to OTHER periods and not for reporting period\n",
    "rest_nonmissing_adsh_otherperiods_only=rest_nonmissing_adsh_otherperiods[rest_nonmissing_adsh_otherperiods.adsh.isin(rest_nonmissing_adsh_reportingperiod.adsh)==False]\n",
    "count_rest_subcase2a = len(rest_nonmissing_adsh_otherperiods_only.drop_duplicates(subset=['adsh'],keep='last'))\n",
    "count_rest_subcase2a\n",
    "# 2.b adshs found that ONLY have records corresponding to the reporting period\n",
    "rest_nonmissing_adsh_reportingperiod_only=rest_nonmissing_adsh_reportingperiod[rest_nonmissing_adsh_reportingperiod.adsh.isin(rest_nonmissing_adsh_otherperiods.adsh)==False]\n",
    "count_rest_subcase2b = len(rest_nonmissing_adsh_reportingperiod_only.drop_duplicates(subset=['adsh'],keep='last'))\n",
    "count_rest_subcase2b\n",
    "# 2.c adshs found that have records for BOTH, the reporting period AND other periods\n",
    "rest_nonmissing_adsh_both_periods=rest_nonmissing_adsh_reportingperiod[rest_nonmissing_adsh_reportingperiod.adsh.isin(rest_nonmissing_adsh_otherperiods.adsh)]\n",
    "count_rest_subcase2c = len(rest_nonmissing_adsh_both_periods.drop_duplicates(subset=['adsh'],keep='last'))\n",
    "count_rest_subcase2c\n",
    "\n",
    "# Naturally Null values\n",
    "rest_nonmissing_adsh_reportingperiod_only_null = rest_nonmissing_adsh_reportingperiod_only[rest_nonmissing_adsh_reportingperiod_only[keyword].isnull()]\n",
    "rest_nonmissing_adsh_both_periods_null = rest_nonmissing_adsh_both_periods[rest_nonmissing_adsh_both_periods[keyword].isnull()]\n",
    "count_rest_subcase2b_null= len(rest_nonmissing_adsh_reportingperiod_only_null.drop_duplicates(subset=['adsh'],keep='last'))\n",
    "count_rest_subcase2c_null= len(rest_nonmissing_adsh_both_periods_null.drop_duplicates(subset=['adsh'],keep='last'))\n",
    "\n",
    "# Revise count of null values since there are cases where:\n",
    "# 1. adsh has some subsidiries reporting null values while other subsidiares have non-null values\n",
    "# 2. adsh has value reported as 0 AND as null.\n",
    "count_rest_subcase2b_null = count_rest_subcase2b_null - len(rest_nonmissing_adsh_reportingperiod_only[rest_nonmissing_adsh_reportingperiod_only.adsh.isin(rest_nonmissing_adsh_reportingperiod_only_null.adsh)].drop_duplicates(subset=['adsh'],keep='last'))\n",
    "count_rest_subcase2c_null = count_rest_subcase2c_null - len(rest_nonmissing_adsh_both_periods[rest_nonmissing_adsh_both_periods.adsh.isin(rest_nonmissing_adsh_both_periods_null.adsh)].drop_duplicates(subset=['adsh'],keep='last'))\n",
    "count_rest_subcase2b_null\n",
    "count_rest_subcase2c_null\n",
    "\n",
    "\n",
    "############ RESULTS\n",
    "# Tables containing adsh records for which we DIDN't get a value\n",
    "nonmatched_adsh = con_missing_adsh.append(rest_missing_adsh)\n",
    "missinginfo_adsh = con_nonmissing_adsh_otherperiods_only.append(rest_nonmissing_adsh_otherperiods_only)\n",
    "nullvalues_adsh = con_nonmissing_adsh_reportingperiod_only_null.append(con_nonmissing_adsh_both_periods_null).append(rest_nonmissing_adsh_reportingperiod_only_null).append(rest_nonmissing_adsh_both_periods_null )\n",
    "\n",
    "unsuccess_adsh = nonmatched_adsh.append(missinginfo_adsh).append(nullvalues_adsh)\n",
    "len(unsuccess_adsh.drop_duplicates(subset=['adsh'],keep='last'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tables containing adsh records were we are able to get a value\n",
    "# Note 1: some records with field_values=0 might also appear as NAN. Keep both.\n",
    "# Note 2: some coregs have values while others might not, for the same (cik,period) record.\n",
    "# Hence tables unsuccess_adsh and success_adsh might have common adshs. This potential double counting is taken care of in the stats: there records (adsh) are counted as part of the non-missing non-null records.\n",
    "con_nonmissing_adsh_reportingperiod_only = con_nonmissing_adsh_reportingperiod_only[con_nonmissing_adsh_reportingperiod_only[keyword].isnull()==False]\n",
    "con_nonmissing_adsh_both_periods = con_nonmissing_adsh_both_periods[con_nonmissing_adsh_both_periods[keyword].isnull()==False]\n",
    "rest_nonmissing_adsh_reportingperiod_only = rest_nonmissing_adsh_reportingperiod_only[rest_nonmissing_adsh_reportingperiod_only[keyword].isnull()==False]\n",
    "rest_nonmissing_adsh_both_periods = rest_nonmissing_adsh_both_periods[rest_nonmissing_adsh_both_periods[keyword].isnull()==False]\n",
    "\n",
    "success_adsh = con_nonmissing_adsh_reportingperiod_only.append(con_nonmissing_adsh_both_periods).append(rest_nonmissing_adsh_reportingperiod_only).append(rest_nonmissing_adsh_both_periods)\n",
    "len(success_adsh.drop_duplicates(subset=['adsh'],keep='last'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009 Baseline: Using (adsh) to match records from SUB to NUM Files\n",
      "Total adsh records in SUB files: 31\n",
      "Total adsh records matched from SUB to NUM files: 16/31\n",
      "Number adsh records matched with NO info in NUM file about reporting period: 0/16\n",
      "Number adsh records in NUM files with Null values for Liabilities: 0/16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "########### SUMMARY STATS\n",
    "count_matched = count_con_subcase2 + count_rest_subcase2\n",
    "count_missing_info = count_con_subcase2a + count_rest_subcase2a\n",
    "count_nonmissing_info = count_con_subcase2b + count_con_subcase2c + count_rest_subcase2b + count_rest_subcase2c\n",
    "count_nonmissing_info_null = count_con_subcase2b_null + count_con_subcase2c_null + count_rest_subcase2b_null + count_rest_subcase2c_null\n",
    "count_total_nonvalues = count_total-count_matched+count_missing_info+count_nonmissing_info_null\n",
    "count_total_values = count_matched - count_missing_info - count_nonmissing_info_null\n",
    "\n",
    "# Print summary stats\n",
    "print(str(year)+\" Baseline: Using (adsh) to match records from SUB to NUM Files\")\n",
    "print(\"Total adsh records in SUB files: \"+str(count_total))\n",
    "print(\"Total adsh records matched from SUB to NUM files: \"+str(count_matched)+\"/\"+str(count_total))\n",
    "print(\"Number adsh records matched with NO info in NUM file about reporting period: \"+str(count_missing_info)+\"/\"+str(count_matched))\n",
    "print(\"Number adsh records in NUM files with Null values for \"+str(keyword)+\": \"+str(count_nonmissing_info_null)+\"/\"+str(count_nonmissing_info))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### GET VALUES FROM NUM FILE MATCHING (CIK,PERIOD) FROM SUB FILE #########\n",
    "# Drop unsuccess_adsh duplicate records by adsh (i.e. keep list of unmatched adsh or equivalently (cik,period) and keep only relevant fields:\n",
    "unsuccess_adsh_unique = unsuccess_adsh.drop_duplicates(subset=['adsh'],keep='last')[['adsh','cik','fy','period']]\n",
    "len(unsuccess_adsh_unique)\n",
    "len(unsuccess_adsh_unique.drop_duplicates(subset=['cik','period'],keep='last'))\n",
    "\n",
    "# Left merge similar adsh case, except that now merge is on cik (i.e. adding values in NUM to unsuccess_adsh_unique):\n",
    "# Number of entries in unsuccess_adsh_unique increases as multiple values with the same cik get stacked.\n",
    "unsuccess_adsh_newtry = unsuccess_adsh_unique.merge(val[['adsh','cik','ddate','coreg',keyword]],on='cik',how='left')\n",
    "len(unsuccess_adsh_newtry)\n",
    "len(unsuccess_adsh_newtry.drop_duplicates(subset=['adsh_x'],keep='last')) # Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proceed just like in adsh case for the accounting:\n",
    "# Subcases:\n",
    "#1. No (cik,period) record in NUM file (null ddate)\n",
    "unsuccess_adsh_newtry_missing = unsuccess_adsh_newtry[unsuccess_adsh_newtry['ddate'].isnull()]\n",
    "count_unsuccess_adsh_newtry_missing = len(unsuccess_adsh_newtry_missing.drop_duplicates(subset=['cik','period'],keep='last'))\n",
    "count_unsuccess_adsh_newtry_missing\n",
    "#2. (cik,period) record in NUM file\n",
    "unsuccess_adsh_newtry_nonmissing = unsuccess_adsh_newtry[unsuccess_adsh_newtry['ddate'].isnull()==False]\n",
    "count_unsuccess_adsh_newtry_nonmissing = len(unsuccess_adsh_newtry_nonmissing.drop_duplicates(subset=['cik','period'],keep='last'))\n",
    "count_unsuccess_adsh_newtry_nonmissing\n",
    "# Subsubcases\n",
    "# (cik,period) records found in NUM file that contain information about the reporting period\n",
    "unsuccess_adsh_newtry_nonmissing_reportingperiod=unsuccess_adsh_newtry_nonmissing[unsuccess_adsh_newtry_nonmissing.period==unsuccess_adsh_newtry_nonmissing.ddate]\n",
    "len(unsuccess_adsh_newtry_nonmissing_reportingperiod)\n",
    "# (cik,period) records found in NUM file that contain information about periods other than the reporting period\n",
    "unsuccess_adsh_newtry_nonmissing_otherperiods=unsuccess_adsh_newtry_nonmissing[unsuccess_adsh_newtry_nonmissing.period!=unsuccess_adsh_newtry_nonmissing.ddate]\n",
    "len(unsuccess_adsh_newtry_nonmissing_otherperiods)\n",
    "# Count RECORDS in each partition using adsh_x (record identifier from unsucess_10k_adshunique) easier to use than (cik,period)\n",
    "# 2.a. (adsh_x) that ONLY have records corresponding to OTHER periods and not for reporting period\n",
    "unsuccess_adsh_newtry_nonmissing_otherperiods_only=unsuccess_adsh_newtry_nonmissing_otherperiods[unsuccess_adsh_newtry_nonmissing_otherperiods.adsh_x.isin(unsuccess_adsh_newtry_nonmissing_reportingperiod.adsh_x)==False]\n",
    "count_unsuccess_adsh_newtry_nonmissing_otherperiods_only = len(unsuccess_adsh_newtry_nonmissing_otherperiods_only.drop_duplicates(subset=['cik','period'],keep='last'))\n",
    "count_unsuccess_adsh_newtry_nonmissing_otherperiods_only\n",
    "# 2.b (adsh_x) that ONLY have records corresponding to the reporting period\n",
    "unsuccess_adsh_newtry_nonmissing_reportingperiod_only=unsuccess_adsh_newtry_nonmissing_reportingperiod[unsuccess_adsh_newtry_nonmissing_reportingperiod.adsh_x.isin(unsuccess_adsh_newtry_nonmissing_otherperiods.adsh_x)==False]\n",
    "count_unsuccess_adsh_newtry_nonmissing_reportingperiod_only = len(unsuccess_adsh_newtry_nonmissing_reportingperiod_only.drop_duplicates(subset=['cik','period'],keep='last'))\n",
    "count_unsuccess_adsh_newtry_nonmissing_reportingperiod_only\n",
    "# 2.c (adsh_x) that have records for BOTH, the reporting period AND other periods\n",
    "unsuccess_adsh_newtry_nonmissing_both_periods=unsuccess_adsh_newtry_nonmissing_reportingperiod[unsuccess_adsh_newtry_nonmissing_reportingperiod.adsh_x.isin(unsuccess_adsh_newtry_nonmissing_otherperiods.adsh_x)]\n",
    "count_unsuccess_adsh_newtry_nonmissing_both_periods = len(unsuccess_adsh_newtry_nonmissing_both_periods.drop_duplicates(subset=['cik','period'],keep='last'))\n",
    "count_unsuccess_adsh_newtry_nonmissing_both_periods\n",
    "\n",
    "# Naturally Null values\n",
    "unsuccess_adsh_newtry_nonmissing_reportingperiod_only_null = unsuccess_adsh_newtry_nonmissing_reportingperiod_only[unsuccess_adsh_newtry_nonmissing_reportingperiod_only[keyword].isnull()]\n",
    "unsuccess_adsh_newtry_nonmissing_both_periods_null = unsuccess_adsh_newtry_nonmissing_both_periods[unsuccess_adsh_newtry_nonmissing_both_periods[keyword].isnull()]\n",
    "count_unsuccess_adsh_newtry_nonmissing_reportingperiod_only_null = len(unsuccess_adsh_newtry_nonmissing_reportingperiod_only_null.drop_duplicates(subset=['cik','period'],keep='last'))\n",
    "count_unsuccess_adsh_newtry_nonmissing_both_periods_null = len(unsuccess_adsh_newtry_nonmissing_both_periods_null.drop_duplicates(subset=['cik','period'],keep='last'))\n",
    "count_unsuccess_adsh_newtry_nonmissing_reportingperiod_only_null\n",
    "count_unsuccess_adsh_newtry_nonmissing_both_periods_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############ RESULTS\n",
    "# Tables containing (cik,period) records where we are unable to get a value after the new try\n",
    "newtry_nonmatched_adsh = unsuccess_adsh_newtry_missing\n",
    "newtry_missinginfo_adsh = unsuccess_adsh_newtry_nonmissing_otherperiods_only\n",
    "newtry_nullvalues_adsh = unsuccess_adsh_newtry_nonmissing_reportingperiod_only_null.append(unsuccess_adsh_newtry_nonmissing_both_periods_null)\n",
    "\n",
    "newtry_unsuccess_adsh = newtry_nonmatched_adsh.append(newtry_missinginfo_adsh).append(newtry_nullvalues_adsh)\n",
    "len(newtry_unsuccess_adsh.drop_duplicates(subset=['cik','period'],keep='last'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tables containing (cik,period) records where we are able to get a value\n",
    "# Note 1: some records with field_values=0 might also appear as NAN. Keep both.\n",
    "# Note 2: some coregs have values while others might not, for the same (cik,period) record.\n",
    "# Hence tables newtry_unsuccess_adsh and newtry_success_adsh might have common (cik,period) records. This potential double counting is taken care of in the stats: there records (adsh) are counted as part of the non-missing non-null records.\n",
    "unsuccess_adsh_newtry_nonmissing_reportingperiod_only = unsuccess_adsh_newtry_nonmissing_reportingperiod_only[unsuccess_adsh_newtry_nonmissing_reportingperiod_only[keyword].isnull()==False]\n",
    "unsuccess_adsh_newtry_nonmissing_both_periods = unsuccess_adsh_newtry_nonmissing_both_periods[unsuccess_adsh_newtry_nonmissing_both_periods[keyword].isnull()==False]\n",
    "\n",
    "newtry_success_adsh = unsuccess_adsh_newtry_nonmissing_reportingperiod_only.append(unsuccess_adsh_newtry_nonmissing_both_periods)\n",
    "len(newtry_success_adsh.drop_duplicates(subset=['cik','period'],keep='last'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Revise count of null values since there are cases were a cik,period (i.e adsh record) has some subsidiries reporting null values while other subsidiares have non-null values\n",
    "count_unsuccess_adsh_newtry_nonmissing_reportingperiod_only_null = count_unsuccess_adsh_newtry_nonmissing_reportingperiod_only_null - len(unsuccess_adsh_newtry_nonmissing_reportingperiod_only[unsuccess_adsh_newtry_nonmissing_reportingperiod_only.adsh_x.isin(unsuccess_adsh_newtry_nonmissing_reportingperiod_only_null.adsh_x)].drop_duplicates(subset=['cik','period'],keep='last'))\n",
    "count_unsuccess_adsh_newtry_nonmissing_both_periods_null = count_unsuccess_adsh_newtry_nonmissing_both_periods_null - len(unsuccess_adsh_newtry_nonmissing_both_periods[unsuccess_adsh_newtry_nonmissing_both_periods.adsh_x.isin(unsuccess_adsh_newtry_nonmissing_both_periods_null.adsh_x)].drop_duplicates(subset=['cik','period'],keep='last'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_unsuccess_adsh_newtry_nonmissing_reportingperiod_only_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_unsuccess_adsh_newtry_nonmissing_both_periods_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adsh_x</th>\n",
       "      <th>cik</th>\n",
       "      <th>fy</th>\n",
       "      <th>period</th>\n",
       "      <th>adsh_y</th>\n",
       "      <th>ddate</th>\n",
       "      <th>coreg</th>\n",
       "      <th>Liabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [adsh_x, cik, fy, period, adsh_y, ddate, coreg, Liabilities]\n",
       "Index: []"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsuccess_adsh_newtry_nonmissing_both_periods[unsuccess_adsh_newtry_nonmissing_both_periods.cik.isin(['92679'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########### SUMMARY STATS\n",
    "newtry_count_matched = count_unsuccess_adsh_newtry_nonmissing\n",
    "newtry_count_missing_info = count_unsuccess_adsh_newtry_nonmissing_otherperiods_only\n",
    "newtry_count_nonmissing_info = count_unsuccess_adsh_newtry_nonmissing_reportingperiod_only + count_unsuccess_adsh_newtry_nonmissing_both_periods\n",
    "newtry_count_nonmissing_info_null = count_unsuccess_adsh_newtry_nonmissing_reportingperiod_only_null + count_unsuccess_adsh_newtry_nonmissing_both_periods_null\n",
    "newtry_count_total_values = newtry_count_matched - newtry_count_missing_info - newtry_count_nonmissing_info_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009 Refinement # 1: Using (cik,period) to match records from SUB to NUM Files\n",
      "Total number of records with no values after baseline (adsh) matching: 15/31\n",
      "Number of records matched using (cik,period): 1/15\n",
      "Number of records matched with NO info in NUM file about reporting period: 1/1\n",
      "Number of records matched with info in NUM file but with NULL values for Liabilities: 0/0\n",
      "Total number of Liabilities values missing: 15/31\n",
      "Total number of Liabilities values recovered: 16/31\n"
     ]
    }
   ],
   "source": [
    "# Print summary stats\n",
    "print(str(year)+\" Refinement # 1: Using (cik,period) to match records from SUB to NUM Files\")\n",
    "print(\"Total number of records with no values after baseline (adsh) matching: \"+str(count_total_nonvalues)+\"/\"+str(count_total))\n",
    "print(\"Number of records matched using (cik,period): \"+str(newtry_count_matched)+\"/\"+str(count_total_nonvalues))\n",
    "print(\"Number of records matched with NO info in NUM file about reporting period: \"+str(newtry_count_missing_info)+\"/\"+str(newtry_count_matched))\n",
    "print(\"Number of records matched with info in NUM file but with NULL values for \"+str(keyword)+\": \"+str(newtry_count_nonmissing_info_null)+\"/\"+str(newtry_count_nonmissing_info))\n",
    "\n",
    "# Final Summay stats: adsh+(cik,period) mismatching\n",
    "print(\"Total number of \"+str(keyword)+\" values missing: \"+str(count_total_nonvalues-newtry_count_matched + newtry_count_missing_info + newtry_count_nonmissing_info_null)+\"/\"+str(count_total))\n",
    "print(\"Total number of \"+str(keyword)+\" values recovered: \"+str(count_total_values+newtry_count_total_values)+\"/\"+str(count_total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################## OUTPUT TABLES ###################################\n",
    "newtry_success_adsh = newtry_success_adsh.rename(columns={'adsh_x':'adsh_10k','adsh_y':'adsh_other'})\\\n",
    "[['adsh_10k','adsh_other','cik','coreg','period','fy','ddate',keyword]]\n",
    "success_adsh = success_adsh.rename(columns={'adsh':'adsh_10k'})\n",
    "\n",
    "# data = success_adsh.append(newtry_success_adsh)\n",
    "# data = success_adsh.append(newtry_success_adsh).append(newtry_unsuccess_adsh)\n",
    "data = success_adsh.append(newtry_success_adsh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Liabilities</th>\n",
       "      <th>adsh_10k</th>\n",
       "      <th>adsh_other</th>\n",
       "      <th>cik</th>\n",
       "      <th>coreg</th>\n",
       "      <th>ddate</th>\n",
       "      <th>ein</th>\n",
       "      <th>fy</th>\n",
       "      <th>period</th>\n",
       "      <th>qtrs</th>\n",
       "      <th>sic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.129000e+09</td>\n",
       "      <td>0000950123-09-057827</td>\n",
       "      <td>NaN</td>\n",
       "      <td>804328</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20090930</td>\n",
       "      <td>953685934</td>\n",
       "      <td>2009</td>\n",
       "      <td>20090930</td>\n",
       "      <td>0</td>\n",
       "      <td>3663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.531100e+09</td>\n",
       "      <td>0000950123-09-064772</td>\n",
       "      <td>NaN</td>\n",
       "      <td>829224</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20090930</td>\n",
       "      <td>911325671</td>\n",
       "      <td>2009</td>\n",
       "      <td>20090930</td>\n",
       "      <td>0</td>\n",
       "      <td>5810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8.099100e+09</td>\n",
       "      <td>0000950123-09-066095</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2969</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20090930</td>\n",
       "      <td>231274455</td>\n",
       "      <td>2009</td>\n",
       "      <td>20090930</td>\n",
       "      <td>0</td>\n",
       "      <td>2810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.215700e+07</td>\n",
       "      <td>0000950123-09-066100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1222333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20090930</td>\n",
       "      <td>522369757</td>\n",
       "      <td>2009</td>\n",
       "      <td>20090930</td>\n",
       "      <td>0</td>\n",
       "      <td>1040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.479635e+09</td>\n",
       "      <td>0000950123-09-070205</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20091031</td>\n",
       "      <td>941655526</td>\n",
       "      <td>2009</td>\n",
       "      <td>20091031</td>\n",
       "      <td>0</td>\n",
       "      <td>3674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.259900e+10</td>\n",
       "      <td>0001047469-09-010234</td>\n",
       "      <td>NaN</td>\n",
       "      <td>833444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20090930</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>20090930</td>\n",
       "      <td>0</td>\n",
       "      <td>7380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>9.194000e+09</td>\n",
       "      <td>0001047469-09-010297</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1385157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20090930</td>\n",
       "      <td>980518048</td>\n",
       "      <td>2009</td>\n",
       "      <td>20090930</td>\n",
       "      <td>0</td>\n",
       "      <td>5065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5.106000e+09</td>\n",
       "      <td>0001047469-09-010861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1090872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20091031</td>\n",
       "      <td>770518772</td>\n",
       "      <td>2009</td>\n",
       "      <td>20091031</td>\n",
       "      <td>0</td>\n",
       "      <td>3825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3.631390e+10</td>\n",
       "      <td>0001104659-09-070344</td>\n",
       "      <td>NaN</td>\n",
       "      <td>315189</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20091031</td>\n",
       "      <td>362382580</td>\n",
       "      <td>2009</td>\n",
       "      <td>20091031</td>\n",
       "      <td>0</td>\n",
       "      <td>3523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5.020440e+08</td>\n",
       "      <td>0001193125-09-179839</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1002638</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20090630</td>\n",
       "      <td>980154400</td>\n",
       "      <td>2009</td>\n",
       "      <td>20090630</td>\n",
       "      <td>0</td>\n",
       "      <td>7373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.601900e+10</td>\n",
       "      <td>0001193125-09-214859</td>\n",
       "      <td>NaN</td>\n",
       "      <td>320193</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20090930</td>\n",
       "      <td>942404110</td>\n",
       "      <td>2009</td>\n",
       "      <td>20090930</td>\n",
       "      <td>0</td>\n",
       "      <td>3571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>9.138000e+09</td>\n",
       "      <td>0001193125-09-239236</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1385187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20090930</td>\n",
       "      <td>980624794</td>\n",
       "      <td>2009</td>\n",
       "      <td>20090930</td>\n",
       "      <td>0</td>\n",
       "      <td>3841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>9.088000e+09</td>\n",
       "      <td>0001193125-09-239249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1403161</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20090930</td>\n",
       "      <td>260267673</td>\n",
       "      <td>2009</td>\n",
       "      <td>20090930</td>\n",
       "      <td>0</td>\n",
       "      <td>7389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.768902e+09</td>\n",
       "      <td>0001193125-09-241304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38777</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20090930</td>\n",
       "      <td>132670991</td>\n",
       "      <td>2009</td>\n",
       "      <td>20090930</td>\n",
       "      <td>0</td>\n",
       "      <td>6282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>9.964650e+08</td>\n",
       "      <td>0001193125-09-242405</td>\n",
       "      <td>NaN</td>\n",
       "      <td>203527</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20090930</td>\n",
       "      <td>942359345</td>\n",
       "      <td>2009</td>\n",
       "      <td>20090930</td>\n",
       "      <td>0</td>\n",
       "      <td>3845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>3.420900e+09</td>\n",
       "      <td>0001437749-09-001934</td>\n",
       "      <td>NaN</td>\n",
       "      <td>353944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20090930</td>\n",
       "      <td>880173041</td>\n",
       "      <td>2009</td>\n",
       "      <td>20090930</td>\n",
       "      <td>0</td>\n",
       "      <td>3990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Liabilities              adsh_10k adsh_other      cik coreg     ddate  \\\n",
       "3   7.129000e+09  0000950123-09-057827        NaN   804328   NaN  20090930   \n",
       "6   2.531100e+09  0000950123-09-064772        NaN   829224   NaN  20090930   \n",
       "11  8.099100e+09  0000950123-09-066095        NaN     2969   NaN  20090930   \n",
       "13  1.215700e+07  0000950123-09-066100        NaN  1222333   NaN  20090930   \n",
       "16  2.479635e+09  0000950123-09-070205        NaN     6951   NaN  20091031   \n",
       "18  1.259900e+10  0001047469-09-010234        NaN   833444   NaN  20090930   \n",
       "20  9.194000e+09  0001047469-09-010297        NaN  1385157   NaN  20090930   \n",
       "24  5.106000e+09  0001047469-09-010861        NaN  1090872   NaN  20091031   \n",
       "26  3.631390e+10  0001104659-09-070344        NaN   315189   NaN  20091031   \n",
       "30  5.020440e+08  0001193125-09-179839        NaN  1002638   NaN  20090630   \n",
       "32  2.601900e+10  0001193125-09-214859        NaN   320193   NaN  20090930   \n",
       "35  9.138000e+09  0001193125-09-239236        NaN  1385187   NaN  20090930   \n",
       "37  9.088000e+09  0001193125-09-239249        NaN  1403161   NaN  20090930   \n",
       "40  1.768902e+09  0001193125-09-241304        NaN    38777   NaN  20090930   \n",
       "43  9.964650e+08  0001193125-09-242405        NaN   203527   NaN  20090930   \n",
       "46  3.420900e+09  0001437749-09-001934        NaN   353944   NaN  20090930   \n",
       "\n",
       "          ein    fy    period qtrs   sic  \n",
       "3   953685934  2009  20090930    0  3663  \n",
       "6   911325671  2009  20090930    0  5810  \n",
       "11  231274455  2009  20090930    0  2810  \n",
       "13  522369757  2009  20090930    0  1040  \n",
       "16  941655526  2009  20091031    0  3674  \n",
       "18          0  2009  20090930    0  7380  \n",
       "20  980518048  2009  20090930    0  5065  \n",
       "24  770518772  2009  20091031    0  3825  \n",
       "26  362382580  2009  20091031    0  3523  \n",
       "30  980154400  2009  20090630    0  7373  \n",
       "32  942404110  2009  20090930    0  3571  \n",
       "35  980624794  2009  20090930    0  3841  \n",
       "37  260267673  2009  20090930    0  7389  \n",
       "40  132670991  2009  20090930    0  6282  \n",
       "43  942359345  2009  20090930    0  3845  \n",
       "46  880173041  2009  20090930    0  3990  "
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 16 entries, 3 to 46\n",
      "Data columns (total 11 columns):\n",
      "Liabilities    16 non-null float64\n",
      "adsh_10k       16 non-null object\n",
      "adsh_other     0 non-null object\n",
      "cik            16 non-null object\n",
      "coreg          0 non-null object\n",
      "ddate          16 non-null object\n",
      "ein            16 non-null object\n",
      "fy             16 non-null object\n",
      "period         16 non-null object\n",
      "qtrs           16 non-null object\n",
      "sic            16 non-null object\n",
      "dtypes: float64(1), object(10)\n",
      "memory usage: 1.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# values = values.rename(columns={'ddate':'period', 'adsh':'adsh_10k'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# values = values.copy()[values.copy().tag==keyword]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# del data['qtrs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fin_data = data.merge(values.loc[:,['adsh_10k','period','qtrs']],on=['adsh_10k','period'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fin_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data.to_csv('esteban_data'+str(year)+keyword+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv('esteban_data'+'_notes_'+str(year)+keyword+'.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
